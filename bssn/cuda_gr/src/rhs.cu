// generated by bssnUtils.py code for derivative computation
//date: 2018-09-06 10:19:01
#include "rhs.cuh"

namespace cuda {

/**compute derivative kernel __computeDerivPass1 */
__global__ void __computeDerivPass1(const double**__unzipInVar,MemoryDerivs* __derivWorkspace, const cuda::_Block* __dendroBlkList, const cudaDeviceProp*__deviceProperties){

const _Block dblock=__dendroBlkList[blockIdx.x];
const unsigned int NUM_SM_UNITS=__deviceProperties->multiProcessorCount;
const unsigned int SM_ID=blockIdx.x%NUM_SM_UNITS;
const unsigned int offset=dblock.getOffset();
const unsigned int *sz=dblock.getSz();
const double* hx=dblock.getDx();
const double dx=hx[0];
const double dy=hx[1];
const double dz=hx[2];
const double* ptmin=dblock.getPtMin();
const double* ptmax=dblock.getPtMax();
const unsigned int bflag=dblock.getBFlag();

const unsigned int blkSz=sz[0]*sz[1]*sz[2];

const unsigned int tile_sz[3]={9,9,9};
const unsigned int TILE_SZ=tile_sz[0]*tile_sz[1]*tile_sz[2];


const unsigned int BLK_INTERATIONS = 1;//ceil((double)sz[0]/tile_sz[0]);
unsigned int ijk_lm[3*2];
//allocate memory for shared deriv variables. 
__shared__ double grad_0[729];
__shared__ double grad_1[729];
__shared__ double grad_2[729];
__shared__ double grad2_0_1[729];
__shared__ double grad2_0_2[729];
__shared__ double grad2_1_2[729];


//allocate memory for shared unzip input. 
__shared__ double unzipVarInShared[729];
for(unsigned int iter=0;iter<BLK_INTERATIONS;iter++){

		 ijk_lm[2*0+0]=max(1,(int)(1 + tile_sz[0]*iter -4));
		 ijk_lm[2*0+1]=min(ijk_lm[2*0+0]+tile_sz[0],sz[0]-1);

		 ijk_lm[2*1+0]=max(1,(int)(1 + tile_sz[1]*iter -4));
		 ijk_lm[2*1+1]=min(ijk_lm[2*1+0]+tile_sz[1],sz[1]-1);

		 ijk_lm[2*2+0]=max(1,(int)(1 + tile_sz[2]*iter -4));
		 ijk_lm[2*2+1]=min(ijk_lm[2*2+0]+tile_sz[2],sz[2]-1);


		//load input data from global to shared memory
		//printf("SM_ID: %d loc: %d value: %f\n",SM_ID,(__derivWorkspace->__maxBlkSz),__derivWorkspace->__grad_0_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]);
		cuda::__loadGlobalToShared<double>(&__unzipInVar[cuda::VAR::U_ALPHA][offset],(double *) unzipVarInShared,(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		__syncthreads();
		deriv42_x((double *) grad_0,(const double *) unzipVarInShared,dx, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		deriv42_y((double *) grad_1,(const double *) unzipVarInShared,dy, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		deriv42_z((double *) grad_2,(const double *) unzipVarInShared,dz, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		__syncthreads();

		deriv42_y((double *) grad2_0_1,(const double *) grad_0,dy, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		deriv42_z((double *) grad2_0_2,(const double *) grad_0,dz, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		deriv42_z((double *) grad2_1_2,(const double *) grad_1,dz, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		__syncthreads();

		//store derivs from shared to global memory
		cuda::__storeSharedToGlobal<double>((double *) grad_0,&(__derivWorkspace->__grad_0_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__storeSharedToGlobal<double>((double *) grad_1,&(__derivWorkspace->__grad_1_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__storeSharedToGlobal<double>((double *) grad_2,&(__derivWorkspace->__grad_2_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__storeSharedToGlobal<double>((double *) grad2_0_1,&(__derivWorkspace->__grad2_0_1_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__storeSharedToGlobal<double>((double *) grad2_0_2,&(__derivWorkspace->__grad2_0_2_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__storeSharedToGlobal<double>((double *) grad2_1_2,&(__derivWorkspace->__grad2_1_2_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		} // end of block tile loop

} // end of kernel __computeDerivPass1


/**compute derivative kernel __computeDerivPass2 */
__global__ void __computeDerivPass2(const double**__unzipInVar,MemoryDerivs* __derivWorkspace, const cuda::_Block* __dendroBlkList, const cudaDeviceProp*__deviceProperties){

const _Block dblock=__dendroBlkList[blockIdx.x];
const unsigned int NUM_SM_UNITS=__deviceProperties->multiProcessorCount;
const unsigned int SM_ID=blockIdx.x%NUM_SM_UNITS;
const unsigned int offset=dblock.getOffset();
const unsigned int *sz=dblock.getSz();
const double* hx=dblock.getDx();
const double dx=hx[0];
const double dy=hx[1];
const double dz=hx[2];
const double* ptmin=dblock.getPtMin();
const double* ptmax=dblock.getPtMax();
const unsigned int bflag=dblock.getBFlag();

const unsigned int blkSz=sz[0]*sz[1]*sz[2];

const unsigned int tile_sz[3]={9,9,9};
const unsigned int TILE_SZ=tile_sz[0]*tile_sz[1]*tile_sz[2];


const unsigned int BLK_INTERATIONS = ceil((double)sz[0]/tile_sz[0]);
unsigned int ijk_lm[3*2];
//allocate memory for shared deriv variables. 
__shared__ double grad2_0_0[729];
__shared__ double grad2_1_1[729];
__shared__ double grad2_2_2[729];
__shared__ double kograd_0[729];
__shared__ double kograd_1[729];
__shared__ double kograd_2[729];


//allocate memory for shared unzip input. 
__shared__ double unzipVarInShared[729];
for(unsigned int iter=0;iter<BLK_INTERATIONS;iter++){

		 ijk_lm[2*0+0]=max(1,(int)(1 + tile_sz[0]*iter -4));
		 ijk_lm[2*0+1]=min(ijk_lm[2*0+0]+tile_sz[0],sz[0]-1);

		 ijk_lm[2*1+0]=max(1,(int)(1 + tile_sz[1]*iter -4));
		 ijk_lm[2*1+1]=min(ijk_lm[2*1+0]+tile_sz[1],sz[1]-1);

		 ijk_lm[2*2+0]=max(1,(int)(1 + tile_sz[2]*iter -4));
		 ijk_lm[2*2+1]=min(ijk_lm[2*2+0]+tile_sz[2],sz[2]-1);


		//load input data from global to shared memory
		cuda::__loadGlobalToShared<double>(&__unzipInVar[cuda::VAR::U_ALPHA][offset],(double *) unzipVarInShared,(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		__syncthreads();
		deriv42_xx((double *) grad2_0_0,(const double *) unzipVarInShared,dx, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		deriv42_yy((double *) grad2_1_1,(const double *) unzipVarInShared,dy, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		deriv42_zz((double *) grad2_2_2,(const double *) unzipVarInShared,dz, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		ko_deriv42_x((double *) kograd_0,(const double *) unzipVarInShared,dx,(const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		ko_deriv42_y((double *) kograd_1,(const double *) unzipVarInShared,dy,(const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		ko_deriv42_z((double *) kograd_2,(const double *) unzipVarInShared,dz,(const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, 2, bflag);
		__syncthreads();

		//store derivs from shared to global memory
		cuda::__storeSharedToGlobal<double>((double *) grad2_0_0,&(__derivWorkspace->__grad2_0_0_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__storeSharedToGlobal<double>((double *) grad2_1_1,&(__derivWorkspace->__grad2_1_1_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__storeSharedToGlobal<double>((double *) grad2_2_2,&(__derivWorkspace->__grad2_2_2_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__storeSharedToGlobal<double>((double *) kograd_0,&(__derivWorkspace->__kograd_0_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__storeSharedToGlobal<double>((double *) kograd_1,&(__derivWorkspace->__kograd_1_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__storeSharedToGlobal<double>((double *) kograd_2,&(__derivWorkspace->__kograd_2_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		} // end of block tile loop

} // end of kernel __computeDerivPass2


/**compute derivative kernel __computeDerivPass3 */
__global__ void __computeDerivPass3(const double**__unzipInVar,MemoryDerivs* __derivWorkspace, const cuda::_Block* __dendroBlkList, const cudaDeviceProp*__deviceProperties){

const _Block dblock=__dendroBlkList[blockIdx.x];
const unsigned int NUM_SM_UNITS=__deviceProperties->multiProcessorCount;
const unsigned int SM_ID=blockIdx.x%NUM_SM_UNITS;
const unsigned int offset=dblock.getOffset();
const unsigned int *sz=dblock.getSz();
const double* hx=dblock.getDx();
const double dx=hx[0];
const double dy=hx[1];
const double dz=hx[2];
const double* ptmin=dblock.getPtMin();
const double* ptmax=dblock.getPtMax();
const unsigned int bflag=dblock.getBFlag();

const unsigned int blkSz=sz[0]*sz[1]*sz[2];

const unsigned int tile_sz[3]={11,11,11};
const unsigned int TILE_SZ=tile_sz[0]*tile_sz[1]*tile_sz[2];


const unsigned int BLK_INTERATIONS = ceil((double)sz[0]/tile_sz[0]);
unsigned int ijk_lm[3*2];
//allocate memory for shared deriv variables. 
__shared__ double agrad_0[1331];


//allocate memory for shared unzip input. 
__shared__ double unzipVarInShared[1331];
__shared__ double beta0[1331];
for(unsigned int iter=0;iter<BLK_INTERATIONS;iter++){

		 ijk_lm[2*0+0]=max(0,(int)(0 + tile_sz[0]*iter -6));
		 ijk_lm[2*0+1]=min(ijk_lm[2*0+0]+tile_sz[0],sz[0]);

		 ijk_lm[2*1+0]=max(0,(int)(0 + tile_sz[1]*iter -6));
		 ijk_lm[2*1+1]=min(ijk_lm[2*1+0]+tile_sz[1],sz[1]);

		 ijk_lm[2*2+0]=max(0,(int)(0 + tile_sz[2]*iter -6));
		 ijk_lm[2*2+1]=min(ijk_lm[2*2+0]+tile_sz[2],sz[2]);


		//load input data from global to shared memory
		cuda::__loadGlobalToShared<double>(&__unzipInVar[cuda::VAR::U_ALPHA][offset],(double *) unzipVarInShared,(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__loadGlobalToShared<double>(&__unzipInVar[cuda::VAR::U_BETA0][offset], (double *) beta0, (const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		__syncthreads();
		deriv42adv_x((double *) agrad_0,(const double *) unzipVarInShared,dx, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, (const double*) beta0 , 3, bflag);
		__syncthreads();

		//store derivs from shared to global memory
		cuda::__storeSharedToGlobal<double>((double *) agrad_0,&(__derivWorkspace->__agrad_0_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		} // end of block tile loop

} // end of kernel __computeDerivPass3


/**compute derivative kernel __computeDerivPass4 */
__global__ void __computeDerivPass4(const double**__unzipInVar,MemoryDerivs* __derivWorkspace, const cuda::_Block* __dendroBlkList, const cudaDeviceProp*__deviceProperties){

const _Block dblock=__dendroBlkList[blockIdx.x];
const unsigned int NUM_SM_UNITS=__deviceProperties->multiProcessorCount;
const unsigned int SM_ID=blockIdx.x%NUM_SM_UNITS;
const unsigned int offset=dblock.getOffset();
const unsigned int *sz=dblock.getSz();
const double* hx=dblock.getDx();
const double dx=hx[0];
const double dy=hx[1];
const double dz=hx[2];
const double* ptmin=dblock.getPtMin();
const double* ptmax=dblock.getPtMax();
const unsigned int bflag=dblock.getBFlag();

const unsigned int blkSz=sz[0]*sz[1]*sz[2];

const unsigned int tile_sz[3]={11,11,11};
const unsigned int TILE_SZ=tile_sz[0]*tile_sz[1]*tile_sz[2];


const unsigned int BLK_INTERATIONS = ceil((double)sz[0]/tile_sz[0]);
unsigned int ijk_lm[3*2];
//allocate memory for shared deriv variables. 
__shared__ double agrad_1[1331];


//allocate memory for shared unzip input. 
__shared__ double unzipVarInShared[1331];
__shared__ double beta0[1331];
for(unsigned int iter=0;iter<BLK_INTERATIONS;iter++){

		 ijk_lm[2*0+0]=max(0,(int)(0 + tile_sz[0]*iter -6));
		 ijk_lm[2*0+1]=min(ijk_lm[2*0+0]+tile_sz[0],sz[0]);

		 ijk_lm[2*1+0]=max(0,(int)(0 + tile_sz[1]*iter -6));
		 ijk_lm[2*1+1]=min(ijk_lm[2*1+0]+tile_sz[1],sz[1]);

		 ijk_lm[2*2+0]=max(0,(int)(0 + tile_sz[2]*iter -6));
		 ijk_lm[2*2+1]=min(ijk_lm[2*2+0]+tile_sz[2],sz[2]);


		//load input data from global to shared memory
		cuda::__loadGlobalToShared<double>(&__unzipInVar[cuda::VAR::U_ALPHA][offset],(double *) unzipVarInShared,(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__loadGlobalToShared<double>(&__unzipInVar[cuda::VAR::U_BETA1][offset], (double *) beta0, (const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		__syncthreads();
		deriv42adv_y((double *) agrad_1,(const double *) unzipVarInShared,dy, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, (const double*) beta0 , 3, bflag);
		__syncthreads();

		//store derivs from shared to global memory
		cuda::__storeSharedToGlobal<double>((double *) agrad_1,&(__derivWorkspace->__agrad_1_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		} // end of block tile loop

} // end of kernel __computeDerivPass4


/**compute derivative kernel __computeDerivPass5 */
__global__ void __computeDerivPass5(const double**__unzipInVar,MemoryDerivs* __derivWorkspace, const cuda::_Block* __dendroBlkList, const cudaDeviceProp*__deviceProperties){

const _Block dblock=__dendroBlkList[blockIdx.x];
const unsigned int NUM_SM_UNITS=__deviceProperties->multiProcessorCount;
const unsigned int SM_ID=blockIdx.x%NUM_SM_UNITS;
const unsigned int offset=dblock.getOffset();
const unsigned int *sz=dblock.getSz();
const double* hx=dblock.getDx();
const double dx=hx[0];
const double dy=hx[1];
const double dz=hx[2];
const double* ptmin=dblock.getPtMin();
const double* ptmax=dblock.getPtMax();
const unsigned int bflag=dblock.getBFlag();

const unsigned int blkSz=sz[0]*sz[1]*sz[2];

const unsigned int tile_sz[3]={11,11,11};
const unsigned int TILE_SZ=tile_sz[0]*tile_sz[1]*tile_sz[2];


const unsigned int BLK_INTERATIONS = ceil((double)sz[0]/tile_sz[0]);
unsigned int ijk_lm[3*2];
//allocate memory for shared deriv variables. 
__shared__ double agrad_2[1331];


//allocate memory for shared unzip input. 
__shared__ double unzipVarInShared[1331];
__shared__ double beta0[1331];
for(unsigned int iter=0;iter<BLK_INTERATIONS;iter++){

		 ijk_lm[2*0+0]=max(0,(int)(0 + tile_sz[0]*iter -6));
		 ijk_lm[2*0+1]=min(ijk_lm[2*0+0]+tile_sz[0],sz[0]);

		 ijk_lm[2*1+0]=max(0,(int)(0 + tile_sz[1]*iter -6));
		 ijk_lm[2*1+1]=min(ijk_lm[2*1+0]+tile_sz[1],sz[1]);

		 ijk_lm[2*2+0]=max(0,(int)(0 + tile_sz[2]*iter -6));
		 ijk_lm[2*2+1]=min(ijk_lm[2*2+0]+tile_sz[2],sz[2]);


		//load input data from global to shared memory
		cuda::__loadGlobalToShared<double>(&__unzipInVar[cuda::VAR::U_ALPHA][offset],(double *) unzipVarInShared,(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		cuda::__loadGlobalToShared<double>(&__unzipInVar[cuda::VAR::U_BETA2][offset], (double *) beta0, (const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		__syncthreads();
		deriv42adv_z((double *) agrad_2,(const double *) unzipVarInShared,dz, (const unsigned int *) ijk_lm , (const unsigned int *) sz , (const unsigned int *) tile_sz, (const double*) beta0 , 3, bflag);
		__syncthreads();

		//store derivs from shared to global memory
		cuda::__storeSharedToGlobal<double>((double *) agrad_2,&(__derivWorkspace->__agrad_2_alpha[SM_ID * (__derivWorkspace->__maxBlkSz)]),(const unsigned int *) ijk_lm,(const unsigned int *) sz,(const unsigned int *) tile_sz);
		} // end of block tile loop

} // end of kernel __computeDerivPass5


}// end of namespace cuda
